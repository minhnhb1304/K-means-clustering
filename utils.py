import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score


# ===========================
# T·∫¢I V√Ä TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU
# ===========================

def load_data(uploaded_file):
    """
    T·∫£i d·ªØ li·ªáu t·ª´ file CSV ƒë√£ upload.
    
    Tham s·ªë:
        uploaded_file: ƒê·ªëi t∆∞·ª£ng UploadedFile c·ªßa Streamlit
        
    Tr·∫£ v·ªÅ:
        pd.DataFrame: DataFrame ƒë√£ t·∫£i
    """
    return pd.read_csv(uploaded_file)


def get_numeric_columns(df):
    """
    L·∫•y t·∫•t c·∫£ c√°c c·ªôt s·ªë t·ª´ dataframe.
    
    Tham s·ªë:
        df: pandas DataFrame
        
    Tr·∫£ v·ªÅ:
        list: Danh s√°ch t√™n c√°c c·ªôt s·ªë
    """
    return df.select_dtypes(include=[np.number]).columns.tolist()


def get_default_features(numeric_columns):
    """
    L·∫•y c√°c ƒë·∫∑c tr∆∞ng m·∫∑c ƒë·ªãnh ƒë·ªÉ ph√¢n c·ª•m (quantity, n_review, avg_rating n·∫øu c√≥).
    
    Tham s·ªë:
        numeric_columns: Danh s√°ch t√™n c√°c c·ªôt s·ªë
        
    Tr·∫£ v·ªÅ:
        list: Danh s√°ch t√™n c√°c ƒë·∫∑c tr∆∞ng m·∫∑c ƒë·ªãnh
    """
    default_features = [col for col in ['quantity', 'n_review', 'avg_rating'] 
                       if col in numeric_columns]
    return default_features if default_features else numeric_columns[:3]


def preprocess_data(df, selected_features):
    """
    Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu: lo·∫°i b·ªè gi√° tr·ªã thi·∫øu v√† chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng.
    
    Tham s·ªë:
        df: pandas DataFrame
        selected_features: Danh s√°ch t√™n ƒë·∫∑c tr∆∞ng ƒë·ªÉ s·ª≠ d·ª•ng
        
    Tr·∫£ v·ªÅ:
        tuple: (df_processed, X_scaled, scaler, df_scaled, rows_removed)
    """
    # T·∫°o b·∫£n sao
    df_processed = df.copy()
    
    # ƒê·∫øm s·ªë d√≤ng tr∆∞·ªõc khi x·ª≠ l√Ω
    rows_before = df_processed.shape[0]
    
    # Lo·∫°i b·ªè c√°c d√≤ng c√≥ gi√° tr·ªã thi·∫øu trong c√°c ƒë·∫∑c tr∆∞ng ƒë√£ ch·ªçn
    df_processed = df_processed.dropna(subset=selected_features)
    
    # T√≠nh s·ªë d√≤ng ƒë√£ lo·∫°i b·ªè
    rows_removed = rows_before - df_processed.shape[0]
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_processed[selected_features])
    
    # T·∫°o dataframe ƒë√£ chu·∫©n h√≥a ƒë·ªÉ hi·ªÉn th·ªã
    df_scaled = pd.DataFrame(
        X_scaled,
        columns=[f"{col}_scaled" for col in selected_features],
        index=df_processed.index
    )
    
    return df_processed, X_scaled, scaler, df_scaled, rows_removed


# ===========================
# PH∆Ø∆†NG PH√ÅP ELBOW V√Ä ƒê√ÅNH GI√Å
# ===========================

@st.cache_data
def calculate_elbow_method(X_scaled, k_range=(1, 11)):
    """
    T√≠nh to√°n inertia v√† silhouette scores cho c√°c gi√° tr·ªã K kh√°c nhau.
    ƒê∆∞·ª£c cache ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t.
    
    Tham s·ªë:
        X_scaled: Ma tr·∫≠n ƒë·∫∑c tr∆∞ng ƒë√£ chu·∫©n h√≥a
        k_range: Tuple c·ªßa (min_k, max_k)
        
    Tr·∫£ v·ªÅ:
        tuple: (K_range, inertia_values, silhouette_scores)
    """
    inertia_values = []
    silhouette_scores = []
    K_range = range(k_range[0], k_range[1])
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertia_values.append(kmeans.inertia_)
        
        # T√≠nh Silhouette Score (ch·ªâ cho k >= 2)
        if k >= 2:
            score = silhouette_score(X_scaled, kmeans.labels_)
            silhouette_scores.append(score)
        else:
            silhouette_scores.append(0)
    
    return K_range, inertia_values, silhouette_scores


# ===========================
# PH√ÇN C·ª§M K-MEANS
# ===========================

def train_kmeans(X_scaled, n_clusters):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh K-Means v√† tr·∫£ v·ªÅ d·ª± ƒëo√°n.
    
    Tham s·ªë:
        X_scaled: Ma tr·∫≠n ƒë·∫∑c tr∆∞ng ƒë√£ chu·∫©n h√≥a
        n_clusters: S·ªë l∆∞·ª£ng c·ª•m
        
    Tr·∫£ v·ªÅ:
        tuple: (kmeans_model, cluster_labels)
    """
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    return kmeans, cluster_labels


def add_cluster_labels_to_df(df, cluster_labels):
    """
    Th√™m nh√£n c·ª•m v√†o dataframe.
    
    Tham s·ªë:
        df: pandas DataFrame
        cluster_labels: M·∫£ng c√°c nh√£n c·ª•m
        
    Tr·∫£ v·ªÅ:
        pd.DataFrame: DataFrame v·ªõi c·ªôt 'Cluster'
    """
    df_copy = df.copy()
    df_copy['Cluster'] = cluster_labels
    df_copy['Cluster'] = df_copy['Cluster'].astype(str)
    return df_copy


def calculate_cluster_statistics(df_processed, selected_features):
    """
    T√≠nh to√°n th·ªëng k√™ trung b√¨nh cho m·ªói c·ª•m.
    
    Tham s·ªë:
        df_processed: DataFrame c√≥ nh√£n c·ª•m
        selected_features: Danh s√°ch ƒë·∫∑c tr∆∞ng ƒë·ªÉ t√≠nh th·ªëng k√™
        
    Tr·∫£ v·ªÅ:
        pd.DataFrame: Th·ªëng k√™ theo t·ª´ng c·ª•m
    """
    cluster_stats = df_processed.groupby('Cluster')[selected_features].mean()
    cluster_stats = cluster_stats.round(2)
    return cluster_stats


# ===========================
# PH√ÇN T√çCH V√Ä G√ÅN NH√ÉN C·ª§M
# ===========================

def calculate_global_averages(df, features):
    """
    T√≠nh trung b√¨nh to√†n c·ª•c cho c√°c ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh.
    
    Tham s·ªë:
        df: pandas DataFrame
        features: Danh s√°ch t√™n ƒë·∫∑c tr∆∞ng
        
    Tr·∫£ v·ªÅ:
        dict: Dictionary c·ªßa feature: gi√°_tr·ªã_trung_b√¨nh
    """
    return {feature: df[feature].mean() for feature in features}


def identify_trend_cluster(df_processed, quantity_col='quantity'):
    """
    X√°c ƒë·ªãnh c·ª•m c√≥ l∆∞·ª£ng b√°n trung b√¨nh cao nh·∫•t (c·ª•m xu h∆∞·ªõng).
    
    Tham s·ªë:
        df_processed: DataFrame c√≥ nh√£n c·ª•m
        quantity_col: T√™n c·ªôt quantity
        
    Tr·∫£ v·ªÅ:
        str: ID c·ªßa c·ª•m c√≥ l∆∞·ª£ng b√°n trung b√¨nh cao nh·∫•t
    """
    cluster_avg_qty = df_processed.groupby('Cluster')[quantity_col].mean()
    return cluster_avg_qty.idxmax()


def get_cluster_label(cluster_id, trend_cluster_id, mean_qty, mean_rating, 
                     avg_qty_all, avg_rating_all):
    """
    √Åp d·ª•ng logic g√°n nh√£n ƒë·ªÉ x√°c ƒë·ªãnh nh√£n v√† m√†u cho c·ª•m.
    
    Tham s·ªë:
        cluster_id: ID c·ªßa c·ª•m hi·ªán t·∫°i
        trend_cluster_id: ID c·ªßa c·ª•m xu h∆∞·ªõng
        mean_qty: L∆∞·ª£ng b√°n trung b√¨nh c·ªßa c·ª•m hi·ªán t·∫°i
        mean_rating: Rating trung b√¨nh c·ªßa c·ª•m hi·ªán t·∫°i
        avg_qty_all: L∆∞·ª£ng b√°n trung b√¨nh to√†n c·ª•c
        avg_rating_all: Rating trung b√¨nh to√†n c·ª•c
        
    Tr·∫£ v·ªÅ:
        tuple: (label, label_color)
    """
    if cluster_id == trend_cluster_id:
        label = "üî• NH√ìM XU H∆Ø·ªöNG (TRENDING - B√°n Ch·∫°y Nh·∫•t)"
        label_color = "#ff4b4b"
    elif mean_qty < avg_qty_all and mean_rating >= avg_rating_all:
        label = "üíé NH√ìM TI·ªÄM NƒÇNG (B√°n √≠t nh∆∞ng Rating r·∫•t cao)"
        label_color = "#00cc88"
    elif mean_qty < avg_qty_all and mean_rating < avg_rating_all:
        label = "‚ö†Ô∏è NH√ìM C·∫¶N C·∫¢I THI·ªÜN (Hi·ªáu su·∫•t th·∫•p)"
        label_color = "#ffa500"
    else:
        label = "üìö NH√ìM PH·ªî TH√îNG (B√°n ·ªïn ƒë·ªãnh)"
        label_color = "#0068c9"
    
    return label, label_color


def get_dominant_category(cluster_data, category_col='category'):
    """
    T√¨m th·ªÉ lo·∫°i chi·∫øm ∆∞u th·∫ø trong m·ªôt c·ª•m.
    
    Tham s·ªë:
        cluster_data: DataFrame ch·ª©a d·ªØ li·ªáu c·ª•m
        category_col: T√™n c·ªôt category
        
    Tr·∫£ v·ªÅ:
        tuple: (dominant_category, count, category_info_string) ho·∫∑c (None, 0, "N/A")
    """
    if category_col in cluster_data.columns:
        category_counts = cluster_data[category_col].value_counts()
        dominant_category = category_counts.index[0]
        dominant_count = category_counts.values[0]
        category_info = f"**{dominant_category}** ({dominant_count} s√°ch)"
        return dominant_category, dominant_count, category_info
    else:
        return None, 0, "N/A (kh√¥ng c√≥ c·ªôt category)"


def get_cluster_feature_stats(cluster_data, selected_features):
    """
    T√≠nh to√°n th·ªëng k√™ chi ti·∫øt cho c√°c ƒë·∫∑c tr∆∞ng c·ªßa c·ª•m.
    
    Tham s·ªë:
        cluster_data: DataFrame ch·ª©a d·ªØ li·ªáu c·ª•m
        selected_features: Danh s√°ch t√™n ƒë·∫∑c tr∆∞ng
        
    Tr·∫£ v·ªÅ:
        pd.DataFrame: DataFrame th·ªëng k√™
    """
    stats_df = pd.DataFrame({
        'Ch·ªâ S·ªë': selected_features,
        'Gi√° Tr·ªã TB': [cluster_data[feat].mean() for feat in selected_features],
        'Min': [cluster_data[feat].min() for feat in selected_features],
        'Max': [cluster_data[feat].max() for feat in selected_features]
    })
    stats_df['Gi√° Tr·ªã TB'] = stats_df['Gi√° Tr·ªã TB'].round(2)
    stats_df['Min'] = stats_df['Min'].round(2)
    stats_df['Max'] = stats_df['Max'].round(2)
    return stats_df


def get_category_distribution(cluster_data, top_n=8, category_col='category'):
    """
    L·∫•y ph√¢n b·ªë th·ªÉ lo·∫°i trong m·ªôt c·ª•m.
    
    Tham s·ªë:
        cluster_data: DataFrame ch·ª©a d·ªØ li·ªáu c·ª•m
        top_n: S·ªë l∆∞·ª£ng th·ªÉ lo·∫°i h√†ng ƒë·∫ßu c·∫ßn tr·∫£ v·ªÅ
        category_col: T√™n c·ªôt category
        
    Tr·∫£ v·ªÅ:
        pd.Series: S·ªë l∆∞·ª£ng theo th·ªÉ lo·∫°i (top_n th·ªÉ lo·∫°i)
    """
    if category_col in cluster_data.columns:
        category_counts = cluster_data[category_col].value_counts()
        return category_counts.head(min(top_n, len(category_counts)))
    return None


def prepare_download_data(df):
    """
    Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ t·∫£i xu·ªëng CSV.
    
    Tham s·ªë:
        df: pandas DataFrame
        
    Tr·∫£ v·ªÅ:
        bytes: D·ªØ li·ªáu CSV ƒë√£ m√£ h√≥a
    """
    return df.to_csv(index=False).encode('utf-8')

